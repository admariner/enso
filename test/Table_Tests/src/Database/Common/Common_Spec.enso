from Standard.Base import all
import Standard.Base.Errors.Common.Index_Out_Of_Bounds
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument

from Standard.Table import Table, Sort_Column, Aggregate_Column
from Standard.Table.Errors import all

from Standard.Database import all
from Standard.Database.Errors import all
import Standard.Database.Internal.IR.Create_Column_Descriptor.Create_Column_Descriptor
import Standard.Database.Internal.IR.Query.Query

from Standard.Test import all
import Standard.Test.Suite.Suite_Builder

import project.Database.Common.Default_Ordering_Spec
import project.Database.Common.Names_Length_Limits_Spec
import project.Database.Common.Single_Column_Table_Spec
import Standard.Database.Feature.Feature

import project.Util
import project.Database.Helpers.Name_Generator


upload connection prefix data temporary=True =
    name = Name_Generator.random_name prefix
    table = data.select_into_database_table connection name temporary=temporary primary_key=Nothing
    table


drop_table connection name =
    Panic.catch Any (connection.drop_table name) caught_panic->
        IO.println <| "Failed to drop table: " + name + " because of: " + caught_panic.payload.to_display_text


type Lazy_Ref
    Value ~get

type Basic_Data
    Value ~data

    connection self = self.data.at 0
    t1 self = self.data.at 1
    t2 self = self.data.at 2
    t4 self = self.data.at 3
    big_table self = self.data.at 4
    big_size self = self.data.at 5

    setup ~connection = Basic_Data.Value <|
        big_size = 1000
        t1 = upload connection "T1" (Table.new [["a", [1, 4]], ["b", [2, 5]], ["c", [3, 6]]])
        t2 = upload connection "T2" (Table.new [["d", [100, 200]]])
        ## The effective name may get a deduplication prefix/suffix so we
            need to use `t4.name` instead of the literal string. Still it
            will contain the weird characters we wanted.

            Also, the table name cannot be too long as Postgres truncates at
            63 chars (and we append 37 chars of uniqueness suffix) and the
            test logic will break then.
        t4 = upload connection 'aSELECT "A",\'B\' FROM t;--' (Table.new [["X", ["a", "B"]], ["Y", [2, 5]]])
        big = Table.new [["a", Vector.new big_size ix->ix], ["b", Vector.new big_size ix-> ix *  3.1415926], ["c", Vector.new big_size ix-> ix.to_text]]
        big_table = upload connection "Big" big
        [connection, t1, t2, t4, big_table, big_size]

    teardown self =
        drop_table self.connection self.t1.name
        drop_table self.connection self.t2.name
        drop_table self.connection self.t4.name
        drop_table self.connection self.big_table.name


type Sorting_Data
    Value ~data

    connection self = self.data.at 0
    df self = self.data.at 1
    ints self = self.data.at 2
    reals self = self.data.at 3
    bools self = self.data.at 4
    texts self = self.data.at 5
    t8 self = self.data.at 6

    setup ~connection = Sorting_Data.Value <|
        ints = [1, 2, 3, 4, 5]
        reals = [1.3, 4.6, 3.2, 5.2, 1.6]
        bools = [False, False, True, True, False]
        texts = ["foo", "foo", "bar", "baz", "spam"]
        df = upload connection "clothes" <|
            Table.new [["id", [1,2,3,4,5,6]], ["name", ["shoes","trousers","dress","skirt","blouse","t-shirt"]], ["quantity", [20,10,20,10,30,30]], ["rating", [3.0,Nothing,7.3,3.0,2.2,Nothing]], ["price", [37.2,42.1,64.1,87.4,13.5,64.2]]]
        t8 = upload connection "T8" <|
            Table.new [["ord", [0,3,2,4,1]], ["ints", ints], ["reals", reals], ["bools", bools], ["texts", texts]]
        [connection, df, ints, reals, bools, texts, t8]

    teardown self =
        drop_table self.connection self.df.name
        drop_table self.connection self.t8.name


type Aggregation_Data
    Value ~data

    connection self = self.data.first
    t9 self = self.data.second

    setup ~connection = Aggregation_Data.Value <|
        vecs = Vector.build_multiple 3 builders->
            insert v =
                builders.zip v .append
            insert ["foo",  0.4,     50]
            insert ["foo",  0.2,     10]
            insert ["foo",  0.4,     30]
            insert ["bar",  3.5,     20]
            insert ["foo",  Nothing, 20]
            insert ["baz",  6.7,     40]
            insert ["foo",  Nothing, 10]
            insert ["bar",  97,      60]
            insert ["quux", Nothing, 70]
            insert ["zzzz", Nothing, Nothing]
            insert ["zzzz", 1, 1]
            insert ["zzzz", 0, 0]
            insert ["zzzz", 0, 1]
            insert ["zzzz", 1, 0]
            insert ["zzzz", 0, 0]
            insert ["zzzz", Nothing, Nothing]
        t9 = upload connection "T9" <|
            Table.new [["name", vecs.at 0], ["price", vecs.at 1], ["quantity", vecs.at 2]]
        [connection, t9]

    teardown self =
        drop_table self.connection self.t9.name


type Missing_Values_Data
    Value ~data

    connection self = self.data.first
    t4 self = self.data.second

    setup ~connection = Missing_Values_Data.Value <|
        t4 = upload connection "T4" <|
            Table.new [["a", [0, 1, Nothing, 42, Nothing]], ["b", [True, Nothing, True, False, Nothing]], ["c", ["", "foo", "bar", Nothing, Nothing]], ["rowid", [1, 2, 3, 4, 5]]]
        [connection, t4]

    teardown self =
        drop_table self.connection self.t4.name

add_specs (suite_builder : Suite_Builder) (prefix : Text) (create_connection_fn : (Nothing -> Any)) default_connection setup =
    if setup.is_feature_supported Feature.Integration_Tests then (add_commmon_specs suite_builder prefix create_connection_fn default_connection setup)

## Adds common database tests specs to the suite builder.

   Arguments:
   - create_connection_fn: A function that creates an appropriate Connection to the database backend.
add_commmon_specs (suite_builder : Suite_Builder) (prefix : Text) (create_connection_fn : (Nothing -> Any)) default_connection setup =

    Default_Ordering_Spec.add_specs suite_builder prefix create_connection_fn setup
    Names_Length_Limits_Spec.add_specs suite_builder prefix create_connection_fn
    Single_Column_Table_Spec.add_specs suite_builder prefix setup

    suite_builder.group (prefix + "Basic Table Access") group_builder->
        data = Basic_Data.setup default_connection.get
        
        group_builder.teardown <|
            data.teardown

        group_builder.specify "should allow to materialize tables and columns into local memory" <|
            df = data.t1.read
            a = data.t1.at 'a' . read
            df.at 'a' . to_vector . should_equal [1, 4]
            a.to_vector . should_equal [1, 4]
            
        group_builder.specify "should allow to materialize columns directly into a Vector" <|
            v = data.t1.at 'a' . to_vector
            v . should_equal [1, 4]

        group_builder.specify "should allow getting specific elements" <|
            test_column = data.t1.at 'a'
            test_column.get 0 . should_equal 1
            test_column.get 3 . should_equal Nothing
            test_column.get 4 -1 . should_equal -1

        group_builder.specify "should allow getting specific elements (with at)" <|
            test_column = data.t1.at 'a'
            test_column.at 0 . should_equal 1
            test_column.at 1 . should_equal 4
            test_column.at 3 . should_fail_with Index_Out_Of_Bounds

        group_builder.specify "should handle bigger result sets" <|
            data.big_table.read.row_count . should_equal data.big_size
            
        group_builder.specify "should not allow to set a column coming from another table" <|
            data.t1.set (data.t2.at "d") . should_fail_with Integrity_Error


    suite_builder.group (prefix + "Connection.query") group_builder->
        data = Basic_Data.setup default_connection.get
        
        group_builder.teardown <|
            data.teardown

        group_builder.specify "should allow to access a Table by name" <|
            name = data.t1.name
            tmp = data.connection.query (SQL_Query.Table_Name name)
            tmp.read . should_equal data.t1.read

        group_builder.specify "should allow to access a Table by an SQL query" <|
            name = data.t1.name
            t2 = data.connection.query (SQL_Query.Raw_SQL ('SELECT "a", "b" FROM "' + name + '" WHERE "a" >= 3'))
            m2 = t2.read
            m2.column_names . should_equal ["a", "b"]
            m2.at "a" . to_vector . should_equal [4]
            m2.at "b" . to_vector . should_equal [5]
            m2.at "c" . should_fail_with No_Such_Column

            t3 = data.connection.query (SQL_Query.Raw_SQL ('SELECT 1+2 AS "a"'))
            m3 = t3.read
            m3.at 0 . to_vector . should_equal [3]

        group_builder.specify "should use labels for column names" <|
            name = data.t1.name
            t2 = data.connection.query (SQL_Query.Raw_SQL ('SELECT "a" AS "c", "b" FROM "' + name + '" WHERE "a" >= 3'))
            m2 = t2.read
            m2.column_names . should_equal ["c", "b"]
            m2.at "c" . to_vector . should_equal [4]
            m2.at "b" . to_vector . should_equal [5]
            m2.at "a" . should_fail_with No_Such_Column

        group_builder.specify "should report an error depending on input SQL_Query type" <|
            r2 = data.connection.query (SQL_Query.Table_Name "NONEXISTENT-TABLE")
            r2.should_fail_with Table_Not_Found
            r2.catch.name . should_equal "NONEXISTENT-TABLE"
            r2.catch.to_display_text . should_equal "Table NONEXISTENT-TABLE was not found in the database."

            r3 = data.connection.query (SQL_Query.Raw_SQL "MALFORMED-QUERY")
            r3.should_fail_with SQL_Error

        group_builder.specify "should not allow interpolations in raw user-built queries" <|
            r = data.connection.query (SQL_Query.Raw_SQL "SELECT 1 + ?")
            r.should_fail_with Illegal_Argument

        group_builder.specify "will fail if the table is modified and a column gets removed" <|
            name = Name_Generator.random_name "removing-column"

            t0 = (Table.new [["a", [1, 2, 3]], ["b", [4, 5, 6]]]).select_into_database_table data.connection name temporary=True
            t1 = data.connection.query t0.name
            m1 = t1.read.sort "a"
            Problems.assume_no_problems m1
            m1.at "a" . to_vector . should_equal [1, 2, 3]
            m1.at "b" . to_vector . should_equal [4, 5, 6]

            Problems.assume_no_problems <| data.connection.drop_table t0.name
            Problems.assume_no_problems <|
                (Table.new [["a", [100, 200]]]).select_into_database_table data.connection name temporary=True

            # Reading a column that was kept will work OK
            t1.at "a" . to_vector . should_equal [100, 200]

            # But reading the whole table will fail on the missing column:
            m2 = t1.read
            m2.should_fail_with SQL_Error

        group_builder.specify "will not fail if the table is modified and a column gets added" <|
            name = Name_Generator.random_name "adding-column"

            t0 = (Table.new [["a", [1, 2, 3]], ["b", [4, 5, 6]]]).select_into_database_table data.connection name temporary=True

            t1 = data.connection.query t0.name
            m1 = t1.read.sort "a"
            Problems.assume_no_problems m1
            m1.at "a" . to_vector . should_equal [1, 2, 3]
            m1.at "b" . to_vector . should_equal [4, 5, 6]

            Problems.assume_no_problems <| data.connection.drop_table t0.name
            Problems.assume_no_problems <|
                (Table.new [["a", [100, 200]], ["b", [300, 400]], ["c", [500, 600]]]).select_into_database_table data.connection name temporary=True

            m2 = t1.read
            Problems.assume_no_problems m2
            m2.column_names . should_equal ["a", "b"]
            m2.at "a" . to_vector . should_equal [100, 200]
            m2.at "b" . to_vector . should_equal [300, 400]

            t1.at "c" . should_fail_with No_Such_Column

            t2 = data.connection.query t0.name
            t2.column_names . should_equal ["a", "b", "c"]


    suite_builder.group (prefix + "Masking Tables") group_builder->
        data = Basic_Data.setup default_connection.get
        
        group_builder.teardown <|
            data.teardown

        group_builder.specify "should allow to select rows from a table or column based on an expression" <|
            t2 = data.t1.filter (data.t1.at "a" == 1)
            df = t2.read
            df.at "a" . to_vector . should_equal [1]
            df.at "b" . to_vector . should_equal [2]
            df.at "c" . to_vector . should_equal [3]
            t2.at "a" . to_vector . should_equal [1]
            t2.at "b" . to_vector . should_equal [2]
            t2.at "c" . to_vector . should_equal [3]

    suite_builder.group (prefix + "Missing Values")  group_builder->
        data = Missing_Values_Data.setup default_connection.get
        
        group_builder.teardown <|
            data.teardown

        group_builder.specify "fill_nothing should replace nulls" <|
            t4_ordered = data.t4.sort "rowid"
            t4_ordered.at "a" . fill_nothing 10 . to_vector . should_equal [0, 1, 10, 42, 10]
            t4_ordered.at "b" . fill_nothing False . to_vector . should_equal [True, False, True, False, False]
            t4_ordered.at "c" . fill_nothing "NA" . to_vector . should_equal ["", "foo", "bar", "NA", "NA"]

        group_builder.specify "should correctly be counted" <|
            data.t4.row_count . should_equal 5
            col = data.t4.at 'a'
            col.length . should_equal 5
            col.count . should_equal 3
            col.count_nothing . should_equal 2


    suite_builder.group (prefix + "Sorting") group_builder->
        data = Sorting_Data.setup default_connection.get
        
        group_builder.teardown <|
            data.teardown

        group_builder.specify "should allow sorting by a single column name" <|
            r_1 = data.df.sort ([..Name 'quantity'])
            r_1.at 'id' . to_vector . should_have_relative_ordering [[2,4],[1,3],[5,6]]

            r_3 = data.df.sort ([..Name 'rating' ..Descending])
            r_3.at 'id' . to_vector . should_have_relative_ordering [[3],[1,4],[5],[2,6]]

        group_builder.specify 'should allow sorting by multiple column names' <|
            r_1 = data.df.sort ([..Name 'quantity', ..Name 'rating'])
            r_1.at 'id' . to_vector . should_equal [2,4,1,3,6,5]

            r_2 = data.df.sort ([..Name 'rating' ..Descending, ..Name 'quantity' ..Descending])
            r_2.at 'id' . to_vector . should_equal [3,1,4,5,6,2]


        group_builder.specify 'should allow sorting with specific by-column rules' <|
            r_1 = data.df.sort ([..Name "quantity", ..Name "price" ..Descending])
            r_1.at 'id' . to_vector . should_equal [4,2,3,1,6,5]

        group_builder.specify 'should correctly reorder all kinds of columns and leave the original columns untouched' <|
            initial = data.t8.sort 'ints'
            r = initial.sort ([Sort_Column.Name 'ord'])

            r.at 'ints' . to_vector . should_equal [1, 5, 3, 2, 4]
            initial.at 'ints' . to_vector . should_equal data.ints

            r.at 'reals' . to_vector . should_equal [1.3, 1.6, 3.2, 4.6, 5.2]
            initial.at 'reals' . to_vector . should_equal data.reals

            r.at 'bools' . to_vector . should_equal [False, False, True, False, True]
            initial.at 'bools' . to_vector . should_equal data.bools

            r.at 'texts' . to_vector . should_equal ['foo', 'spam', 'bar', 'foo', 'baz']
            initial.at 'texts' . to_vector . should_equal data.texts

        group_builder.specify 'should sort columns with specified ordering and missing placement' <|
            c = data.df.at 'rating'

            r_1 = c.sort
            r_1.to_vector.should_equal [Nothing, Nothing, 2.2, 3.0, 3.0, 7.3]

            r_2 = c.sort Sort_Direction.Descending
            r_2.to_vector.should_equal [7.3, 3.0, 3.0, 2.2, Nothing, Nothing]


    suite_builder.group prefix+"Aggregation" group_builder->
        data = Aggregation_Data.setup default_connection.get

        group_builder.teardown <|
            data.teardown

        ## A helper which makes sure that the groups in a materialized
           (InMemory) table are ordered according to a specified column or list
           of columns.
        determinize_by order_column table =
            table.sort ([..Name order_column])

        group_builder.specify "should allow counting group sizes and elements" <|
            ## Names set to lower case to avoid issue with Redshift where columns are
               returned in lower case.
            aggregates = [Aggregate_Column.Count "count", Aggregate_Column.Count_Not_Nothing "price" "count not nothing price", Aggregate_Column.Count_Nothing "price" "count nothing price"]

            t1 = determinize_by "name" (data.t9.aggregate ["name"] aggregates . read)
            t1.at  "name" . to_vector . should_equal ["bar", "baz", "foo", "quux", "zzzz"]
            t1.at  "count" . to_vector . should_equal [2, 1, 5, 1, 7]
            t1.at  "count not nothing price" . to_vector . should_equal [2, 1, 3, 0, 5]
            t1.at  "count nothing price" . to_vector . should_equal [0, 0, 2, 1, 2]

            t2 = data.t9.aggregate [] aggregates . read
            t2.at  "count" . to_vector . should_equal [16]
            t2.at  "count not nothing price" . to_vector . should_equal [11]
            t2.at  "count nothing price" . to_vector . should_equal [5]

        group_builder.specify "should allow simple arithmetic aggregations" <|
            ## Names set to lower case to avoid issue with Redshift where columns are
               returned in lower case.
            aggregates = [Aggregate_Column.Sum "price" "sum price", Aggregate_Column.Sum "quantity" "sum quantity", Aggregate_Column.Average "price" "avg price"]
            ## TODO can check the datatypes

            t1 = determinize_by "name" (data.t9.aggregate ["name"] aggregates . read)
            t1.at  "name" . to_vector . should_equal ["bar", "baz", "foo", "quux", "zzzz"]
            t1.at  "sum price" . to_vector . should_equal [100.5, 6.7, 1, Nothing, 2]
            t1.at  "sum quantity" . to_vector . should_equal [80, 40, 120, 70, 2]
            t1.at  "avg price" . to_vector . should_equal [50.25, 6.7, (1/3), Nothing, (2/5)]

            t2 = data.t9.aggregate [] aggregates . read
            t2.at  "sum price" . to_vector . should_equal [110.2]
            t2.at  "sum quantity" . to_vector . should_equal [312]
            t2.at  "avg price" . to_vector . should_equal [(110.2 / 11)]

    suite_builder.group prefix+"Table.filter" group_builder->
        data = Basic_Data.setup default_connection.get

        group_builder.teardown <|
            data.teardown

        group_builder.specify "report error when trying to filter by a custom predicate" <|
            data.t1.filter "a" (x -> x % 2 == 0) . should_fail_with Unsupported_Database_Operation

    suite_builder.group prefix+"Internals" group_builder->
        group_builder.specify "should correctly detect Table Already Exists error" <|
            connection = default_connection.get
            name = Name_Generator.random_name "clashing-table"
            statement = connection.dialect.generate_sql <|
                Query.Create_Table name [Create_Column_Descriptor.Value "x" "CHAR" []] primary_key=Nothing temporary=True

            Problems.assume_no_problems <| connection.execute_update statement

            # Second run should fail now:
            r2 = connection.execute_update statement
            r2.should_fail_with SQL_Error

            # The error should be identified as Table Already Exists
            connection.dialect.get_error_mapper.is_table_already_exists_error r2.catch . should_be_true

            # But other kinds of errors should not:
            other_error = connection.execute_update "invalid; )))) syntax;;;;"
            other_error.should_fail_with SQL_Error
            connection.dialect.get_error_mapper.is_table_already_exists_error other_error.catch . should_be_false
