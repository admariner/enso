from Standard.Base import all
import Standard.Base.Errors.Common.Failed_To_Load
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument

from Standard.Table import all
from Standard.Table.Errors import Invalid_Value_Type, No_Rows
from Standard.Database import all

from Standard.Test import all

from project.Util import all
from project.Common_Table_Operations.Util import within_table

main filter=Nothing =
    suite = Test.build suite_builder->
        add_specs suite_builder
    suite.run_with_filter filter

type Lazy_Ref
    Value ~get

add_specs suite_builder =
    suite_builder.group "Data.read_many" group_builder->
        # One File and one Text path
        files_vector = [enso_project.data / "empty.txt", (enso_project.data / "sample.tsv") . path]
        sample_table = Lazy_Ref.Value <|
            (enso_project.data / "sample.tsv") . read

        check_common_columns table =
            table.at "Value" . to_vector . should_equal ["" , Nothing, Nothing]
            table.at "a" . to_vector     . should_equal [Nothing, 1, 4]
            table.at "b" . to_vector     . should_equal [Nothing, 2, 5]
            table.at "c" . to_vector     . should_equal [Nothing, 3, 6]
        check_returned_vector vec =
            vec.should_be_a Vector
            vec.length . should_equal 2
            vec.first . should_equal ""
            vec.second . should_equal sample_table.get

        group_builder.specify "should read files listed in a Column" <|
            column = Column.from_vector "Col" files_vector
            r1 = Data.read_many column return=..As_Vector
            check_returned_vector r1
            Problems.assume_no_problems r1

            r2 = Data.read_many column return=..With_New_Column
            r2.should_be_a Table
            r2.column_names . should_equal ["Col", "Value"]
            r2.at "Col" . to_vector . should_equal files_vector
            check_returned_vector (r2.at "Value" . to_vector)

        group_builder.specify "should read files listed in a single column Table" <|
            table1 = Table.new [["Some column", files_vector]]
            r1 = Data.read_many table1 return=..As_Vector
            check_returned_vector r1

            r2 = Data.read_many table1 return=..With_New_Column
            r2.should_be_a Table
            r2.column_names . should_equal ["Some column", "Value"]
            r2.at "Some column" . to_vector . should_equal files_vector
            check_returned_vector (r2.at "Value" . to_vector)

        group_builder.specify "should read files listed in a Table with `path` column" <|
            table1 = Table.new [["X", [1, 2]], ["path", files_vector]]
            r1 = Data.read_many table1 return=..As_Vector
            check_returned_vector r1
            Problems.assume_no_problems r1

            r2 = Data.read_many table1 return=..With_New_Column
            r2.should_be_a Table
            r2.column_names . should_equal ["X", "path", "Value"]
            r2.at "X" . to_vector . should_equal [1, 2]
            r2.at "path" . to_vector . should_equal files_vector
            check_returned_vector (r2.at "Value" . to_vector)

            # Test that this is really case insensitive
            table3 = Table.new [["X", [1, 2]], ["pAtH", files_vector]]
            r3 = Data.read_many table3 return=..With_New_Column
            r3.should_be_a Table
            r3.column_names . should_equal ["X", "pAtH", "Value"]
            check_returned_vector (r3.at "Value" . to_vector)

        group_builder.specify "should fail if no `path` column can be found or its ambiguous" <|
            table1 = Table.new [["X", [1, 2]], ["Y", files_vector]]
            r1 = Data.read_many table1 return=..As_Vector
            r1.should_fail_with Illegal_Argument

            table2 = Table.new [["X", [1, 2]], ["path", files_vector], ["Path", [3, 4]]]
            r2 = Data.read_many table2 return=..As_Vector
            r2.should_fail_with Illegal_Argument

        group_builder.specify "fails if a DB Table or Column is provided, telling to materialize first to in-memory" <|
            connection = Database.connect SQLite.In_Memory
            paths_vector = files_vector.map x-> case x of
                f : File -> f.path
                p : Text -> p

            table = (Table.new [["path", paths_vector]]).select_into_database_table connection "test_table" temporary=True
            r = Data.read_many table return=..As_Vector
            r.should_fail_with Illegal_Argument

            col = table.at "path"
            r2 = Data.read_many col return=..As_Vector
            r2.should_fail_with Illegal_Argument

        group_builder.specify "fails if a column of invalid type is provided" <|
            table = Table.new [["path", [1, 2]], ["X", [33, 44]]]

            Data.read_many table . should_fail_with Invalid_Value_Type
            Data.read_many (table.at "path") . should_fail_with Invalid_Value_Type
            Data.read_many (table.select_columns ["X"]) . should_fail_with Invalid_Value_Type

        group_builder.specify "should return a merged table by default" <|
            r1 = Data.read_many (Column.from_vector "my column" files_vector)
            r1.should_be_a Table
            r1.column_names . should_equal ["my column", "Value", "a", "b", "c"]
            r1.at "my column" . to_vector . should_equal [files_vector.first, files_vector.second, files_vector.second]
            check_common_columns r1

            r2 = Data.read_many (Table.new [["X", [100, 200]], ["Path", files_vector], ["Y", [300, 400]]])
            r2.should_be_a Table
            r2.column_names . should_equal ["X", "Path", "Y", "Value", "a", "b", "c"]
            # The second row is duplicated because it gets expanded along with the table that was loaded that has 2 rows
            r2.at "X" . to_vector . should_equal [100, 200, 200]
            r2.at "Y" . to_vector . should_equal [300, 400, 400]
            check_common_columns r2

            r3 = Data.read_many files_vector
            r3.should_be_a Table
            r3.column_names . should_equal ["Path", "Value", "a", "b", "c"]
            check_common_columns r3

        group_builder.specify "if input is a Vector, the default can be overridden to return a new column" <|
            r1 = Data.read_many files_vector return=..With_New_Column
            r1.should_be_a Table
            r1.column_names . should_equal ["Path", "Value"]
            r1.at "Path" . to_vector . should_equal files_vector
            check_returned_vector (r1.at "Value" . to_vector)

        group_builder.specify "should merge files that read as non-Table values into a Table using reasonable defaults" <|
            with_temp_dir base_dir->
                # raw JS Object - we want it to expand to a single row - same as if it was in a 1-element array
                (JS_Object.from_pairs [["a", 1], ["b", 2]]).to_json.write (base_dir / "1_js_object.json")

                # array of JS objects
                [JS_Object.from_pairs [["a", 30], ["b", 40], ["c", "foobar"]], JS_Object.from_pairs [["a", 50], ["b", 60]]].to_json.write (base_dir / "2_js_array.json")

                # JS array of numbers
                [100, 200, 300].to_json.write (base_dir / "3_js_numbers.json")

                # a Table
                (Table.new [["a", [-1, -2]], ["d", [-4, -5]]]).write (base_dir / "4_table.tsv")

                # a plain text value
                "Hi!".write (base_dir / "5_plain_text.txt")

                # JS null
                "null".write (base_dir / "6_js_null.json")

                # a JS string
                '"str"'.write (base_dir / "7_js_string.json")

                files = Data.list base_dir . sort on=(.name)
                r = Data.read_many files
                r.should_be_a Table

                within_table r <|
                    # We transform the Path to just file name
                    rows = (r.set (r.at "Path" . map .name) "Path").rows.map .to_vector

                    null = Nothing
                    r.column_names . should_equal ["Path",               "a",  "b",      "c", "Value",  "d"]
                    rows.at 0 .      should_equal ["1_js_object.json",     1,    2,     null,    null, null]
                    rows.at 1 .      should_equal ["2_js_array.json",     30,   40, "foobar",    null, null]
                    rows.at 2 .      should_equal ["2_js_array.json",     50,   60,     null,    null, null]
                    rows.at 3 .      should_equal ["3_js_numbers.json", null, null,     null,   "100", null]
                    rows.at 4 .      should_equal ["3_js_numbers.json", null, null,     null,   "200", null]
                    rows.at 5 .      should_equal ["3_js_numbers.json", null, null,     null,   "300", null]
                    rows.at 6 .      should_equal ["4_table.tsv",         -1, null,     null,    null,   -4]
                    rows.at 7 .      should_equal ["4_table.tsv",         -2, null,     null,    null,   -5]
                    rows.at 8 .      should_equal ["5_plain_text.txt",  null, null,     null,   "Hi!", null]
                    rows.at 9 .      should_equal ["6_js_null.json",    null, null,     null,    null, null]
                    rows.at 10 .     should_equal ["7_js_string.json",  null, null,     null,   "str", null]

                    r.at "a" . value_type     . should_equal Value_Type.Integer
                    r.at "b" . value_type     . should_equal Value_Type.Integer
                    r.at "c" . value_type     . should_equal Value_Type.Char
                    r.at "Value" . value_type . should_equal Value_Type.Char
                    r.at "d" . value_type     . should_equal Value_Type.Integer

        group_builder.specify "should warn when a file loads as empty array and not include it in the As_Merged_Table result" <|
            # But such array should not influence the columns present:
            with_temp_dir base_dir->
                '{"a": 1}'.write (base_dir / "1_js_object.json")
                "[]".write (base_dir / "2_empty_array.json")
                '[{"a": 2, "b": "..."}]'.write (base_dir / "3_js_object.json")

                files = Data.list base_dir . sort on=(.name)

                r1 = Data.read_many files return=..With_New_Column
                r1.should_be_a Table
                Problems.assume_no_problems r1
                r1.row_count . should_equal 3
                r1 . at "Path" . map .name . to_vector . should_equal ["1_js_object.json", "2_empty_array.json", "3_js_object.json"]
                r1.at "Value" . at 1 . should_equal []

                r2 = Data.read_many files
                r2.should_be_a Table
                w2 = Problems.expect_only_warning No_Rows r2
                w2.to_display_text . should_contain "2_empty_array.json"
                w2.to_display_text . should_contain "loaded as an empty array, so it is not included in the `As_Merged_Table` result of `read_many`."
                within_table r2 <|
                    r2.column_names . should_equal ["Path", "a", "b"]
                    r2.row_count . should_equal 2
                    r2.at "Path" . map .name . to_vector . should_equal ["1_js_object.json", "3_js_object.json"]
                    r2.at "a" . to_vector . should_equal [1, 2]
                    r2.at "b" . to_vector . should_equal [Nothing, "..."]

        group_builder.specify "should warn when a Table loads as 0-rows and is not included in As_Merged_Table result, but it should still influence the result columns" <|
            with_temp_dir base_dir->
                'A,B'.write (base_dir / "1_empty_table.csv")
                'B,C\n1,2'.write (base_dir / "2_table.csv")

                files = Data.list base_dir . sort on=(.name)

                r1 = Data.read_many files format=(..Delimited ',' headers=True) return=..With_New_Column
                r1.should_be_a Table
                Problems.assume_no_problems r1
                r1.row_count . should_equal 2
                r1.at "Path" . map .name . to_vector . should_equal ["1_empty_table.csv", "2_table.csv"]
                empty_table1 = r1.at "Value" . at 0
                empty_table1.should_be_a Table
                empty_table1.row_count . should_equal 0
                empty_table1.column_names . should_equal ["A", "B"]

                r2 = Data.read_many files format=(..Delimited ',' headers=True) return=..As_Merged_Table
                r2.should_be_a Table
                within_table r2 <|
                    ## But it DOES influence the columns present.
                       That is because the column structure is a 'structural' property,
                       we want the structure of the result to be the same regardless if it has 0 or more rows.
                       If the workflow is run next time with this table having some rows, it is better that the structure is preserved.
                       Otherwise, a workflow that is running fine could stop working once a file is changed to contain no rows, as some column could no longer be found.
                    r2.column_names . should_equal ["Path", "A", "B", "C"]
                    r2.row_count . should_equal 1
                    r2.at "Path" . map .name . to_vector . should_equal ["2_table.csv"]
                    r2.at "A" . to_vector . should_equal [Nothing]
                    r2.at "B" . to_vector . should_equal [1]
                    r2.at "C" . to_vector . should_equal [2]

                    w2 = Problems.expect_only_warning No_Rows r2
                    w2.to_display_text . should_contain "1_empty_table.csv"
                    w2.to_display_text . should_contain "loaded as a table with 0 rows, so it did not contribute any rows to the `As_Merged_Table` result of `read_many`."

        group_builder.specify "should allow to customize how the tables are merged" <|
            with_temp_dir base_dir->
                '{"a": 1, "b": 2}'.write (base_dir / "1_js_object.json")
                '{"b": 3, "c": 4}'.write (base_dir / "2_js_object.json")

                files = Data.list base_dir . sort on=(.name)
                
                r1 = Data.read_many files return=(..As_Merged_Table columns_to_keep=..In_All)
                r1.should_be_a Table
                within_table r1 <|
                    r1.column_names . should_equal ["Path", "b"]
                    r1.at "b" . to_vector . should_equal [2, 3]

                r2 = Data.read_many files return=(..As_Merged_Table columns_to_keep=(..In_List ["a", "c"]))
                r2.should_be_a Table
                within_table r2 <|
                    r2.column_names . should_equal ["Path", "a", "c"]
                    r2.at "a" . to_vector . should_equal [1, Nothing]
                    r2.at "c" . to_vector . should_equal [Nothing, 4]

                r3 = Data.read_many files return=(..As_Merged_Table match=..By_Position)
                r3.should_be_a Table
                within_table r3 <|
                    r3.column_names . should_equal ["Path", "a", "b"]
                    r3.at "a" . to_vector . should_equal [1, 3]
                    r3.at "b" . to_vector . should_equal [2, 4]

        group_builder.specify "should fallback to Char if no common type can be found for primitive values" <|
            with_temp_dir base_dir->
                '{"a": 1}'.write (base_dir / "1_js_object.json")
                '{"a": "str"}'.write (base_dir / "2_js_object.json")
                files = Data.list base_dir . sort on=(.name)
                r = Data.read_many files
                r.should_be_a Table
                r.column_names . should_equal ["Path", "a"]
                r.at "a" . value_type . should_equal Value_Type.Char
                r.at "a" . to_vector . should_equal ["1", "str"]

        group_builder.specify "but should keep Mixed type if more complex types are found, like dictionary" <|
            with_temp_dir base_dir->
                '{"a": {}}'.write (base_dir / "1_js_object.json")
                '{"a": []}'.write (base_dir / "2_js_object.json")
                files = Data.list base_dir . sort on=(.name)
                r = Data.read_many files
                r.should_be_a Table
                r.column_names . should_equal ["Path", "a"]
                r.at "a" . value_type . should_equal Value_Type.Mixed
                r.at "a" . to_vector . should_equal [JS_Object.from_pairs [], []]

        group_builder.specify "has sane behaviour if no files were provided" <|
            col = Column.from_vector "C" []
            Data.read_many col return=..As_Vector . should_equal []

            t = Data.read_many col
            t.should_be_a Table
            t.row_count . should_equal 0
            t.column_names . should_equal ["C"]

            t2 = Data.read_many []
            t2.should_be_a Table
            t2.row_count . should_equal 0
            t2.column_names . should_equal ["Path"]

            t3 = Data.read_many [] return=..With_New_Column
            t3.should_be_a Table
            t3.row_count . should_equal 0
            t3.column_names . should_equal ["Path", "Value"]

        group_builder.specify "does not allow Nothing in Path column" <|
            t = Table.new [["A", [1, 2]], ["Path", [Nothing, Nothing]]]
            r1 = Data.read_many t return=..As_Merged_Table
            r1.should_fail_with Illegal_Argument

            r2 = Data.read_many t return=..With_New_Column
            r2.should_fail_with Illegal_Argument

            r3 = Data.read_many t return=..As_Vector
            r3.should_fail_with Illegal_Argument

            r4 = Data.read_many (t.at "Path")
            r4.should_fail_with Illegal_Argument

            r5 = Data.read_many [Nothing]
            r5.should_fail_with Illegal_Argument

        group_builder.specify "in As_Merged_Table mode, will discard rows associated with files that fail to load" <|
            with_temp_dir base_dir->
                'A,B\n1,2'.write (base_dir / "1_table.csv")

                files = [base_dir / "1_table.csv", base_dir / "nonexistent.csv"]
                r = Data.read_many files
                r.should_be_a Table
                r.row_count . should_equal 1
                r.at "Path" . map .name . to_vector . should_equal ["1_table.csv"]

                w = Problems.expect_only_warning No_Rows r
                w.to_display_text . should_contain "nonexistent.csv"
                w.to_display_text . should_contain "does not exist."

                r2 = Data.read_many files return=..With_New_Column
                r2.should_be_a Table
                r2.row_count . should_equal 2
                r2.at "Path" . map .name . to_vector . should_equal ["1_table.csv", "nonexistent.csv"]
                r2.at "Value" . at 0 . should_be_a Table
                r2.at "Value" . at 1 . should_equal Nothing

                w2 = Problems.expect_only_warning Failed_To_Load r2
                w2.to_display_text . should_contain "nonexistent.csv"

        group_builder.specify "should have sane behaviour if all files are weird" <|
            with_temp_dir base_dir->
                '{}'.write (base_dir / "1_js_object.json")
                '[{}, {}]'.write (base_dir / "2_js_array.json")

                files = Data.list base_dir . sort on=(.name)
                r = Data.read_many files
                r.should_be_a Table
                Problems.expect_warning Illegal_Argument r
                within_table r <|
                    r.column_names . should_equal ["Path", "Value"]
                    empty = JS_Object.from_pairs []
                    r.at "Value" . to_vector . should_equal [empty, empty, empty]

            with_temp_dir base_dir->
                '[]'.write (base_dir / "1_empty_array.json")
                '[]'.write (base_dir / "2_empty_array.json")

                files = Data.list base_dir . sort on=(.name)
                r1 = Data.read_many files return=..With_New_Column
                r1.should_be_a Table
                r1.row_count . should_equal 2
                r1.column_names . should_equal ["Path", "Value"]
                Problems.assume_no_problems r1

                r2 = Data.read_many files
                r2.should_be_a Table
                r2.row_count . should_equal 0
                r2.column_names . should_equal ["Path"]
                Problems.expect_only_warning No_Rows r2

        group_builder.specify "should rename duplicated columns, keeping columns from the input unchanged" <|
            tmp_file = enso_project.data / "transient" / "table.csv"
            (Table.new [["Path", [1]], ["Col", [2]]]).write tmp_file on_existing_file=..Overwrite . should_succeed
            Panic.with_finalizer tmp_file.delete <|
                col = Column.from_vector "Col" [tmp_file.path]
                r = Data.read_many col return=..As_Merged_Table
                r.column_names . should_equal ["Col", "Path", "Col 1"]
                r.at "Col" . to_vector . should_equal [tmp_file.path]
                r.at "Path" . to_vector . should_equal [1]
                r.at "Col 1" . to_vector . should_equal [2]

                table = Table.new [["Path", [tmp_file.path]], ["Col", ["X"]], ["Value", ["Y"]]]
                r2 = Data.read_many table return=..As_Merged_Table
                r2.column_names . should_equal ["Path", "Col", "Value", "Path 1", "Col 1"]
                r2.at "Path" . to_vector . should_equal [tmp_file.path]
                r2.at "Col" . to_vector . should_equal ["X"]
                r2.at "Value" . to_vector . should_equal ["Y"]
                r2.at "Path 1" . to_vector . should_equal [1]
                r2.at "Col 1" . to_vector . should_equal [2]

                r3 = Data.read_many table return=..With_New_Column
                r3.column_names . should_equal ["Path", "Col", "Value", "Value 1"]
                r3.at "Path" . to_vector . should_equal [tmp_file.path]
                r3.at "Col" . to_vector . should_equal ["X"]
                r3.at "Value" . to_vector . should_equal ["Y"]
                r3.at "Value 1" . first . should_be_a Table

private with_temp_dir callback =
    base_dir = enso_project.data / "transient" / "read_many_test"
    base_dir.delete_if_exists recursive=True
    base_dir.create_directory . should_succeed
    Panic.with_finalizer (base_dir.delete recursive=True) (callback base_dir)
